# Tuesday, September 5, 2023
How can I use llama models without langchain?

```
def llm_model(max_length=MAX_LENGTH):
    model_name = "meta-llama/Llama-2-7b-chat-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name, load_in_4bit=True, trust_remote_code=True, device_map="auto"
    )
    model.to_bettertransformer()
    generation_pipe = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=max_length,
        trust_remote_code=True,
        device_map="auto",
    )
    return HuggingFacePipeline(pipeline=generation_pipe)

llm = llm_model(max_length=MAX_LENGTH)

result = llm(template)
```


#LLM
#langchain