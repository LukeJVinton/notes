# Friday, December 15, 2023
Still looking into what GPU requirements are for various LLMs. A good article on this is found in https://huggingface.co/blog/optimize-llm
[[LLM vRAM requirements]]

Also interested in the price to performance pay off. What can we afford and what do we need the model to be able to do? 
https://towardsdatascience.com/how-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935
#LLM 

Looking into using lanchain agents to help with document comparisons. However we seem to be running out of memory. Is there a way to stay within the memory when using agents? Limit context length? 